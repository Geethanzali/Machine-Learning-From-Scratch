# Implementation of Neural Network from Scratch

Implementation a single-hidden-layer neural network with a sigmoid activation
function for the hidden layer, and a softmax on the output layer.

DataSet: Optical Character Recognition (OCR) dataset.

Implementation:

1) Intialization of weights and biases: 

RANDOM:  The weights are initialized randomly from a uniform distribution from -0.1 to 0.1.
The bias parameters are initialized to zero.
ZERO:  All weights are initialized to 0.

2) Sigmoid activation function on the hidden layer and softmax on the output layer to ensure it
forms a proper probability distribution.

3) Number of hidden units for the hidden layer is determined by a command line flag

4) Support two different initialization strategies, as described in Section 2.2.1, selecting between them
via a command line flag.

5) Use stochastic gradient descent (SGD) to optimize the parameters for one hidden layer neural network.
The number of epochs will be specified as a command line flag.

6) Set the learning rate via a command line flag.

7) Perform stochastic gradient descent updates on the training data in the order that the data is given
in the input file. 
